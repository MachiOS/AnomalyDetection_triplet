{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import mnist_dataset as mnist_dataset\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_input_fn(data_dir):\n",
    "    \"\"\"Train input function for the MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        data_dir: (string) path to the data directory\n",
    "        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n",
    "    \"\"\"\n",
    "    dataset = mnist_dataset.train(data_dir)\n",
    "#     dataset = dataset.shuffle(params.train_size)  # whole dataset into the buffer\n",
    "#     dataset = dataset.repeat(params.num_epochs)  # repeat for multiple epochs\n",
    "#     dataset = dataset.batch(params.batch_size)\n",
    "    dataset = dataset.prefetch(1)  # make sure you always have one batch ready to serve\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_input_fn(data_dir):\n",
    "    \"\"\"Test input function for the MNIST dataset.\n",
    "\n",
    "    Args:\n",
    "        data_dir: (string) path to the data directory\n",
    "        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n",
    "    \"\"\"\n",
    "    dataset = mnist_dataset.test(data_dir)\n",
    "    \n",
    "    #dataset = dataset.batch(params.batch_size)\n",
    "    dataset = dataset.prefetch(1)  # make sure you always have one batch ready to serve\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(images, label):\n",
    "#     numpy_labels = label.numpy()\n",
    "    return tf.math.equal(label,2)\n",
    "\n",
    "def filter_fn_1(images, label):\n",
    "#     numpy_labels = label.numpy()\n",
    "    return tf.math.equal(label,1)\n",
    "\n",
    "def filter_less_2(images, label):\n",
    "#     numpy_labels = label.numpy()\n",
    "    return tf.math.less(label,2)\n",
    "\n",
    "def filter_less_3(images, label):\n",
    "#     numpy_labels = label.numpy()\n",
    "    return tf.math.less(label,3)\n",
    "\n",
    "def filter_fn_3(images,label):\n",
    "    return tf.math.equal(label,3)\n",
    "\n",
    "\n",
    "def filter_fn_0(images,label):\n",
    "    return tf.math.equal(label,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train_input_fn(\"../data/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = dataset.filter(filter_fn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for image, label in dataset:\n",
    "    i += 1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_input_fn(\"../data/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_2 = dataset.filter(filter_fn)\n",
    "train_1 = dataset.filter(filter_fn_1)\n",
    "train_3 = dataset.filter(filter_fn_3)\n",
    "train_0 = dataset.filter(filter_fn_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new= train_1.concatenate(train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = dataset_new.concatenate(train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subset  =dataset_new.take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-4904d69fe249>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-4904d69fe249>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dataset _subset = dataset_subset.take(10)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dataset _subset = dataset_subset.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.TakeDataset'>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_subset))\n",
    "i = 0\n",
    "for images, labels in dataset_subset:  # only take first element of dataset\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.ConcatenateDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_recover = dataset_new.filter(filter_fn_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5958\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for images, labels in train_2:  # only take first element of dataset\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for images, labels in train_1:  # only take first element of dataset\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for images, labels in train_0:  # only take first element of dataset\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_recover.take(3):  # only take first element of dataset\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    print(numpy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy_labels)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for images,labels in dataset_2:\n",
    "    i += 1\n",
    "\n",
    "print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for images,labels in test_data_2:\n",
    "    i += 1\n",
    "\n",
    "print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2=dataset.filter(filter_fn)\n",
    "print(type(dataset_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in dataset_3.take(1000):  # only take first element of dataset\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    print(numpy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3= dataset_2.filter(filter_fn_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset_2.take(10).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 40\n",
    "channels = [num_channels, num_channels * 2]\n",
    "    \n",
    "# for i, c in enumerate(channels):\n",
    "#     print(i)\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(reversed(channels)):\n",
    "    if i > 0:\n",
    "        print(i)\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.reshape(y_train[0],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read32(bytestream):\n",
    "    \"\"\"Read 4 bytes from bytestream as an unsigned 32-bit integer.\"\"\"\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "\n",
    "def check_image_file_header(filename):\n",
    "    \"\"\"Validate that filename corresponds to images for the MNIST dataset.\"\"\"\n",
    "    with tf.gfile.Open(filename, 'rb') as f:\n",
    "        magic = read32(f)\n",
    "        read32(f)  # num_images, unused\n",
    "        rows = read32(f)\n",
    "        cols = read32(f)\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Invalid magic number %d in MNIST file %s' % (magic, f.name))\n",
    "        if rows != 28 or cols != 28:\n",
    "            raise ValueError(\n",
    "                    'Invalid MNIST file %s: Expected 28x28 images, found %dx%d' %\n",
    "                    (f.name, rows, cols))\n",
    "\n",
    "\n",
    "def check_labels_file_header(filename):\n",
    "    \"\"\"Validate that filename corresponds to labels for the MNIST dataset.\"\"\"\n",
    "    with tf.gfile.Open(filename, 'rb') as f:\n",
    "        magic = read32(f)\n",
    "        read32(f)  # num_items, unused\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Invalid magic number %d in MNIST file %s' % (magic, f.name))\n",
    "\n",
    "\n",
    "def download(directory, filename):\n",
    "    \"\"\"Download (and unzip) a file from the MNIST dataset if not already done.\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if tf.gfile.Exists(filepath):\n",
    "        return filepath\n",
    "    if not tf.gfile.Exists(directory):\n",
    "        tf.gfile.MakeDirs(directory)\n",
    "    # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "    url = 'https://storage.googleapis.com/cvdf-datasets/mnist/' + filename + '.gz'\n",
    "    zipped_filepath = filepath + '.gz'\n",
    "    print('Downloading %s to %s' % (url, zipped_filepath))\n",
    "    urllib.request.urlretrieve(url, zipped_filepath)\n",
    "    with gzip.open(zipped_filepath, 'rb') as f_in, open(filepath, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    os.remove(zipped_filepath)\n",
    "    print(\"filepath {0}\".format(filepath))\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def dataset(directory, images_file, labels_file):\n",
    "    \"\"\"Download and parse MNIST dataset.\"\"\"\n",
    "\n",
    "    images_file = download(directory, images_file)\n",
    "    labels_file = download(directory, labels_file)\n",
    "\n",
    "    check_image_file_header(images_file)\n",
    "    check_labels_file_header(labels_file)\n",
    "\n",
    "    def decode_image(image):\n",
    "        # Normalize from [0, 255] to [0.0, 1.0]\n",
    "        image = tf.decode_raw(image, tf.uint8)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = tf.reshape(image, [784])\n",
    "        return image / 255.0\n",
    "\n",
    "    def decode_label(label):\n",
    "        label = tf.decode_raw(label, tf.uint8)  # tf.string -> [tf.uint8]\n",
    "        label = tf.reshape(label, [])  # label is a scalar\n",
    "        return tf.to_int32(label)\n",
    "\n",
    "    images = tf.data.FixedLengthRecordDataset(images_file, 28 * 28, header_bytes=16)\n",
    "    images = images.map(decode_image)\n",
    "    labels = tf.data.FixedLengthRecordDataset(labels_file, 1, header_bytes=8).map(decode_label)\n",
    "    \n",
    "    return tf.data.Dataset.zip((images, labels))\n",
    "    #return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(directory):\n",
    "    \"\"\"tf.data.Dataset object for MNIST training data.\"\"\"\n",
    "\n",
    "#     (x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "#     x_train = x_train.astype('float32') / 255\n",
    "\n",
    "#     return tf.data.Dataset.zip((images, labels))\n",
    "    return dataset(directory, 'train-images-idx3-ubyte',\n",
    "                   'train-labels-idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  import fashion_mnist\n",
    "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = train(\"/Users/machi/OneDrive/PASTEL Project/QUADL/Phase1/Anomaly_Detection/AE_prototype_for_phase1/mnist-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X,Y in datasets:\n",
    "    print(X,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neko = datasets(\"/Users/machi/OneDrive/PASTEL Project/QUADL/Phase1/Anomaly_Detection/AE_prototype_for_phase1/mnist-data\"'train-images-idx3-ubyte',\n",
    "                   'train-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_2 = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "datasets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in datasets_2:\n",
    "  print(elem.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter(datasets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64 \n",
    "\n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(784, activation='sigmoid'),\n",
    "      layers.Reshape((28, 28))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = Autoencoder(latent_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neko = [1,2,3]\n",
    "np.asarray(neko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.less(neko,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = labels.astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "k=list(compress(neko, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(compress(neko,np.less(neko,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Prd_Normal  Prd_Anormal\n",
      "GT_Normal            1            2\n",
      "GT_Anormal           3            4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame([[1,2],[3,4]],columns=[\"Prd_Normal\",\"Prd_Anormal\"],index=[\"GT_Normal\",\"GT_Anormal\"])\n",
    "# df_results[1]=\n",
    "print(df_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
